\documentclass[journal]{IEEEtran}

\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{multirow}
\usepackage{eurosym}

\title{NATIVA: Unsupervised Bearing Fault Detection Using Spiking Neural Networks with Active Inference}

\author{Saad~LARAJ%
\thanks{Correspondence: saad.laraj@email.com}}

\begin{document}

\maketitle

\begin{abstract}
We present NATIVA, a spiking neural network architecture for unsupervised bearing fault detection. The system combines Leaky Integrate-and-Fire neurons, Spike-Timing-Dependent Plasticity, Kuramoto phase synchronization, and Active Inference (variational Free Energy minimization) to learn normal vibration patterns without labeled data. We evaluate NATIVA on the CWRU bearing dataset across 36 operating conditions (3~fault types, 3~fault sizes, 4~motor loads) and compare against a supervised Random Forest baseline and unsupervised baselines (Isolation Forest, One-Class SVM).

NATIVA achieves a mean AUC-ROC of 0.95 across all conditions, with 0.999 at 0HP and 3HP loads. The system uses a multi-band frequency encoder with global normalization calibrated on healthy data. We identify a performance degradation at 1HP load (AUC~=~0.82) and trace it to harmonic interference between shaft rotation frequency and bearing fault characteristic frequencies. We position NATIVA as a Level-1 wake-up sensor for edge deployment, complementary to supervised diagnostic systems.
\end{abstract}

\begin{IEEEkeywords}
Spiking Neural Networks, Bearing Fault Detection, Active Inference, STDP, Unsupervised Anomaly Detection, Neuromorphic Computing, CWRU
\end{IEEEkeywords}

%-----------------------------------------------------------------------
\section{Introduction}
%-----------------------------------------------------------------------

Bearing failures account for 40--50\% of rotating machinery breakdowns in industry~\cite{henao2014}. Early detection prevents catastrophic failures and costly unplanned downtime. The standard approach uses supervised classifiers (Random Forest, CNN) trained on labeled vibration data with engineered frequency features. This requires labeled examples of each fault type, which are expensive and sometimes impossible to collect for every machine in a fleet.

We explore an alternative: learning what ``normal'' looks like, then flagging any deviation---without ever seeing a fault. This unsupervised paradigm maps naturally onto spiking neural networks, where temporal spike patterns encode signal dynamics and biological learning rules (STDP) enable training without backpropagation.

\subsection*{Contributions}
\begin{enumerate}
    \item A complete SNN architecture combining LIF neurons, STDP, Kuramoto oscillators, and Active Inference for vibration anomaly detection, with explicit mathematical formulation of each component.
    \item A multi-band spike encoding scheme using STFT sub-band decomposition with global normalization, solving the per-window normalization problem that renders SNNs blind to amplitude changes.
    \item A reproducible benchmark on 36 CWRU conditions with honest reporting of failure cases and comparison with both supervised and unsupervised baselines.
\end{enumerate}

%-----------------------------------------------------------------------
\section{Architecture}
%-----------------------------------------------------------------------

\subsection{Overview}

NATIVA processes vibration windows through four stages:
\begin{enumerate}
    \item \textbf{Multi-band encoding}: raw signal $\rightarrow$ STFT $\rightarrow$ 8 frequency bands $\rightarrow$ global normalization $\rightarrow$ binary spike trains
    \item \textbf{Spiking network}: 100 LIF neurons with Winner-Takes-All lateral inhibition and adaptive thresholds
    \item \textbf{Learning}: STDP modulated by Kuramoto phase coherence and Free Energy surprise
    \item \textbf{Anomaly scoring}: mean variational Free Energy (surprise) over the temporal window
\end{enumerate}

\subsection{Multi-Band Spike Encoding}

A vibration window of 1024 samples (85\,ms at 12\,kHz) is decomposed via Short-Time Fourier Transform into a time-frequency representation, then grouped into 8 linearly-spaced frequency bands. For each time step $t$ and band $b$:
\begin{equation}
    E_b(t) = \frac{1}{|B_b|} \sum_{f \in B_b} |Z(f, t)|^2
\end{equation}
where $Z(f,t)$ is the STFT coefficient and $B_b$ is the set of frequency bins in band~$b$.

\textbf{Global normalization.} The critical design choice is calibrating the encoder on healthy data. We compute the 99th percentile of $E_b(t)$ across all training windows for each band:
\begin{equation}
    M_b = \mathrm{percentile}_{99}\!\left(\{E_b(t) : \forall\, t,\ \forall\, \text{window} \in \text{Train}_{\text{normal}}\}\right)
\end{equation}

Spikes are generated deterministically:
\begin{equation}
    s_b(t) = \begin{cases} 1 & \text{if } E_b(t) / M_b > \theta \\ 0 & \text{otherwise} \end{cases}
\end{equation}
with threshold $\theta = 0.15$. This ensures that a faulty bearing---which generates higher-energy harmonics in upper frequency bands---produces proportionally more spikes than a healthy one. Per-window normalization destroys this difference completely (empirically verified: AUC drops from 0.997 to 0.500).

\textbf{On the computational cost of STFT.} We acknowledge that computing a STFT on a microcontroller requires floating-point multiply-accumulate operations, which partially contradicts the event-driven advantage of the downstream SNN. In the current implementation, encoding accounts for approximately 40\% of the total computation. For a fully neuromorphic pipeline, the STFT could be replaced by analog filter banks (silicon cochlea~\cite{liu2010neuromorphic}) or event-driven wavelet decomposition. This remains future work, and the current architecture should be understood as a hybrid (conventional encoding + neuromorphic processing) rather than a purely event-driven system.

\subsection{Neuron Model}

We use standard Leaky Integrate-and-Fire neurons~\cite{gerstner2002}:
\begin{equation}
    \tau_m \frac{dV_j}{dt} = -(V_j - V_{\text{rest}}) + R_m \cdot I_j(t)
\end{equation}
with parameters $\tau_m = 50$\,ms, $V_{\text{th}} = 0.3$, $V_{\text{rest}} = 0$, $R_m = 5.0$. When $V_j \geq V_{\text{th}}$, neuron $j$ emits a spike and enters a refractory period of $t_{\text{ref}} = 2$\,ms.

\textbf{Input current} is computed as a feedforward projection:
\begin{equation}
    I_j(t) = \sum_i W_{ji} \cdot s_i(t) - \alpha_j(t)
\end{equation}
where $W_{ji}$ are the input weights and $\alpha_j(t)$ is the adaptive threshold.

\textbf{Adaptive thresholds}~\cite{diehl2015} prevent individual neuron domination. Each spike increments the threshold:
\begin{equation}
    \alpha_j(t^+) = \alpha_j(t) + \Delta\alpha \quad \text{if neuron } j \text{ fires at time } t
\end{equation}
with $\Delta\alpha = 0.5$ and exponential decay $\alpha_j(t{+}1) = 0.999 \cdot \alpha_j(t)$.

\textbf{Winner-Takes-All} inhibition: at each timestep, only the top-$k = 10$ neurons with highest effective current are allowed to fire, implementing lateral inhibition.

\subsection{STDP Learning}

Input weights are updated via a simplified STDP rule. When neuron $j$ fires at time $t$:
\begin{equation}
    \Delta W_{ji} = A^+ \cdot \phi_j \cdot s_i(t)
\end{equation}
where:
\begin{itemize}
    \item $A^+ = A_{\text{base}} \cdot (0.5 + \sigma)$ is the learning rate, modulated by the Free Energy signal $\sigma$ (defined in Section~II-F)
    \item $\phi_j$ is the phase coherence of neuron $j$ from the Kuramoto module (Section~II-E)
    \item $s_i(t)$ is the input spike at synapse $i$
\end{itemize}

After each update, weights are $L_1$-normalized per row to a target sum of 100, following Diehl \& Cook~\cite{diehl2015}. Weights are clipped to $[0, w_{\max}]$.

This rule can be understood as: neurons that fire strongly to a particular input pattern strengthen their connections to that pattern's active channels, more so when they are phase-synchronized with the population and when the global surprise is high.

\subsection{Kuramoto Phase Synchronization}

$N = 100$ coupled oscillators model the phase dynamics of each neuron. Natural frequencies $\omega_i \sim \mathcal{N}(1.0, 0.1)$ and coupling strength $K = 2.0$. Using the mean-field reduction~\cite{ott2008}:
\begin{equation}
    \dot{\theta}_i = \omega_i + K \cdot R \cdot \sin(\Psi - \theta_i)
\end{equation}
where the complex order parameter is:
\begin{equation}
    R \cdot e^{i\Psi} = \frac{1}{N} \sum_{j=1}^{N} e^{i\theta_j}
\end{equation}
$R \in [0,1]$ measures global synchronization ($R = 1$: fully synchronized, $R = 0$: incoherent). The per-neuron phase coherence used by STDP is:
\begin{equation}
    \phi_j = \frac{1 + \cos(\theta_j - \Psi)}{2} \in [0, 1]
\end{equation}

\textbf{Role of Kuramoto.} The oscillators modulate STDP: neurons that are phase-locked with the population mean field learn faster. This is inspired by the neuroscience finding that gamma-band phase synchronization enhances synaptic plasticity~\cite{fell2011}. In the anomaly detection context, Kuramoto ensures that the network converges to a coherent representation of ``normal'' patterns during calibration.

\subsection{Active Inference: Free Energy Anomaly Score}

The anomaly score is derived from the variational Free Energy principle~\cite{friston2010}. Each neuron $j$ maintains an internal state $\mu_j$ (initialized to 0) that represents its ``expected'' activation. At each timestep, the Free Energy is:
\begin{equation}
    F_j = \underbrace{\frac{1}{2} \pi_o \cdot (o_j - \mu_j)^2}_{\text{Prediction Error}} + \underbrace{\frac{1}{2} \pi_p \cdot (\mu_j - \mu_j^{\text{prior}})^2}_{\text{Prior Divergence}}
    \label{eq:free_energy}
\end{equation}
where:
\begin{itemize}
    \item $o_j \in \{0, 1\}$ is the observed spike output of neuron~$j$ (raw, binary)
    \item $\mu_j$ is the internal state (posterior belief about expected activation)
    \item $\mu_j^{\text{prior}} = 0$ is the prior expectation (initialized to silence)
    \item $\pi_o = 1$ and $\pi_p = 1$ are the observation and prior precisions (fixed, isotropic)
\end{itemize}

\textbf{On applying quadratic Free Energy to binary observations.} A natural objection is that computing a Gaussian prediction error on a binary variable $o_j \in \{0, 1\}$ produces a saccadic signal: the error is large at the exact timestep of a spike and zero otherwise. In standard computational neuroscience, this is handled by filtering $o_j$ through a synaptic trace (exponential low-pass). In our implementation, we do not filter the individual spikes, but instead apply temporal smoothing at the network level. The instantaneous network Free Energy $F(t) = \frac{1}{N}\sum_j F_j(t)$ already averages over $N = 100$ neurons---effectively a spatial low-pass. The temporal smoothing then acts as an explicit causal filter:
\begin{equation}
    \bar{F}(t) = (1-\alpha)\bar{F}(t-1) + \alpha F(t)
\end{equation}
with $\alpha = 0.2$. This two-stage smoothing (spatial average over neurons, then exponential moving average over time) transforms the raw binary prediction errors into a continuous, stable surprise signal. For a window of $T = 64$ timesteps, the anomaly score is:
\begin{equation}
    S = \frac{1}{T} \sum_{t=1}^{T} \bar{F}(t)
\end{equation}

While exponential smoothing introduces a temporal lag, with $\alpha = 0.2$ the effective time constant is $\sim$5 timesteps ($\sim$7\,ms at our resolution), well within the sub-second requirements of industrial predictive maintenance.

The resulting score $S$ is empirically well-behaved: healthy windows produce $S$ with coefficient of variation CV~=~9.3\% ($\sigma/\mu = 0.00022/0.00237$), indicating a stable baseline despite the binary inputs.

\textbf{STDP modulation.} The Free Energy signal also modulates learning. We maintain a running baseline $F_{\text{base}}$ updated with an exponential moving average (rate 0.01). The surprise ratio feeds a sigmoid:
\begin{equation}
    \sigma = \frac{1}{1 + e^{-\beta(F/F_{\text{base}} - 1)}}
\end{equation}
with $\beta = 2.0$. When the current surprise exceeds baseline ($\sigma > 0.5$), STDP learning rate increases; when below, it decreases.

\textbf{Note on the generative model.} The current implementation uses a minimal generative model: the prior is static ($\mu^{\text{prior}} = 0$) and precisions are fixed. A richer parametrization---in particular, adaptive precisions learned from data and a Bernoulli likelihood (more natural for binary spikes than the Gaussian used here)---could improve sensitivity and provide a more principled probabilistic foundation. This is left for future work.

%-----------------------------------------------------------------------
\section{Experimental Setup}
%-----------------------------------------------------------------------

\subsection{Dataset}

We use the Case Western Reserve University (CWRU) Bearing Data Center~\cite{loparo_cwru}, the most widely used benchmark for bearing fault diagnosis. Signals are sampled at 12\,kHz from the drive-end accelerometer. The test bearing is a 6205-2RS JEM SKF.

\textbf{Conditions tested:}
\begin{itemize}
    \item 4 motor loads: 0HP (1797\,RPM), 1HP (1772\,RPM), 2HP (1750\,RPM), 3HP (1730\,RPM)
    \item 3 fault types: Ball, Inner Race, Outer Race (position @6, centered)
    \item 3 fault diameters: 0.007'', 0.014'', 0.021''
    \item 4 normal baselines (one per load)
\end{itemize}

Total: 36 fault conditions + 4 normal baselines. Each recording is segmented into windows of 1024 samples (85.3\,ms) with 50\% overlap, yielding 237--474 windows per condition.

\subsection{Protocol}

For each motor load independently:
\begin{enumerate}
    \item Split normal windows 50/50 (calibration / test)
    \item Calibrate multi-band encoder on the training normal windows (compute $M_b$ per band)
    \item Train NATIVA on the same windows (STDP learning, $\sim$237 windows)
    \item Test on remaining normal windows + all fault windows for that load
    \item Compute AUC-ROC, best-F1 threshold, Precision, Recall, and confusion matrix
\end{enumerate}

This simulates the realistic industrial scenario: the system is deployed on a specific machine, calibrated on its healthy operation at its operating load, then monitors for anomalies.

\subsection{Baselines}

\textbf{Supervised baseline.} A Random Forest classifier (100 trees, 5-fold stratified cross-validation, scikit-learn) trained on 22 hand-crafted features: RMS, peak, crest factor, kurtosis, skewness, standard deviation, mean absolute value, 10 FFT magnitude components, 5 frequency band energies.

\textbf{Unsupervised baselines.} We run three standard unsupervised methods on the same 36 CWRU conditions, using the same per-load calibration protocol (train on healthy windows only, test on healthy~+~fault): \textbf{Dense Autoencoder} ($22{\rightarrow}8{\rightarrow}22$, ReLU, Adam, MSE reconstruction error); \textbf{Isolation Forest} (100 trees, contamination${}=0.01$, scikit-learn); \textbf{One-Class SVM} (RBF kernel, $\nu=0.05$, scikit-learn). All three baselines use the same 22 hand-crafted features as input.

\begin{table}[h]
\centering
\caption{Unsupervised baseline comparison (36 CWRU conditions)}
\label{tab:baselines}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Mean AUC} & \textbf{Min} & \textbf{Max} \\
\midrule
Dense Autoencoder & 1.000 & 1.000 & 1.000 \\
Isolation Forest & 1.000 & 0.999 & 1.000 \\
One-Class SVM & 1.000 & 1.000 & 1.000 \\
\textbf{NATIVA (SNN)} & \textbf{0.951} & \textbf{0.277} & \textbf{1.000} \\
\bottomrule
\end{tabular}
\end{table}

The near-perfect scores of all three baselines reveal that CWRU with 22 FFT features is a trivially separable problem for conventional ML. The AUC gap between NATIVA (0.951) and the baselines (1.000) is attributable to the information bottleneck of spike encoding, not to the learning algorithm. See Section~V-A for discussion.

%-----------------------------------------------------------------------
\section{Results}
%-----------------------------------------------------------------------

\subsection{Single-Condition (0HP, 0.007'')}

\begin{table}[h]
\centering
\caption{Single-condition results (0HP, 0.007'')}
\label{tab:single}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{NATIVA} & \textbf{Random Forest} \\
\midrule
AUC-ROC & 0.997 & 1.000 \\
F1 & 0.992 & 1.000 \\
Recall & 0.989 & 1.000 \\
Precision & 0.994 & 1.000 \\
False Negatives & 8 / 707 & 0 / 707 \\
\bottomrule
\end{tabular}
\end{table}

The statistical separation (d-prime) between healthy and faulty score distributions is $d' = 6.04$, indicating excellent class separability. Healthy windows produce a mean anomaly score $\bar{S}_{\text{normal}} = 0.00237$ (std $= 0.00022$) while faulty windows produce $\bar{S}_{\text{fault}} = 0.00372$ (std $= 0.00027$).

\subsection{Multi-Condition (36 conditions)}

\begin{table}[h]
\centering
\caption{AUC-ROC by motor load}
\label{tab:load}
\begin{tabular}{cccr}
\toprule
\textbf{Load} & \textbf{RPM} & \textbf{AUC (mean $\pm$ std)} & \textbf{N} \\
\midrule
0HP & 1797 & $0.999 \pm 0.002$ & 9 \\
1HP & 1772 & $0.823 \pm 0.236$ & 9 \\
2HP & 1750 & $0.985 \pm 0.030$ & 9 \\
3HP & 1730 & $0.999 \pm 0.003$ & 9 \\
\midrule
\textbf{All} & --- & $\mathbf{0.951 \pm 0.140}$ & \textbf{36} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{AUC-ROC by fault type}
\label{tab:type}
\begin{tabular}{lc}
\toprule
\textbf{Fault Type} & \textbf{AUC (mean $\pm$ std)} \\
\midrule
Inner Race & $0.985 \pm 0.049$ \\
Ball & $0.939 \pm 0.124$ \\
Outer Race & $0.931 \pm 0.199$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{AUC-ROC by fault size}
\label{tab:size}
\begin{tabular}{lc}
\toprule
\textbf{Fault Size} & \textbf{AUC (mean $\pm$ std)} \\
\midrule
0.007'' & $0.977 \pm 0.065$ \\
0.014'' & $0.912 \pm 0.199$ \\
0.021'' & $0.965 \pm 0.114$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Failure Analysis at 1HP}

Three conditions at 1HP load show AUC below 0.77 (Table~\ref{tab:failures}).

\begin{table}[h]
\centering
\caption{Failed conditions at 1HP}
\label{tab:failures}
\begin{tabular}{lcl}
\toprule
\textbf{Condition} & \textbf{AUC} & \textbf{Failure Mode} \\
\midrule
Outer Race 0.014'' & 0.277 & Score inversion \\
Ball 0.021'' & 0.588 & Insufficient separation \\
Ball 0.007'' & 0.764 & Marginal separation \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Root cause analysis.} At 1HP, the shaft rotates at 1772\,RPM ($f_r = 29.53$\,Hz). For the SKF 6205-2RS bearing used in CWRU, the characteristic fault frequencies are:
\begin{itemize}
    \item \textbf{BPFO} = $3.585 \times f_r$ = \textbf{105.9\,Hz}
    \item \textbf{BPFI} = $5.415 \times f_r$ = \textbf{159.9\,Hz}
    \item \textbf{BSF} = $2.357 \times f_r$ = \textbf{69.6\,Hz}
    \item \textbf{FTF} = $0.398 \times f_r$ = \textbf{11.8\,Hz}
\end{itemize}

Our encoder uses 8 linearly-spaced bands covering 0--6000\,Hz, giving a band width of 750\,Hz. At this resolution, the fault frequencies and their first harmonics ($2\times\text{BPFO} = 211.8$\,Hz, $2\times\text{BPFI} = 319.8$\,Hz) all fall within the first band (0--750\,Hz), which is also where the vast majority of normal operating energy resides.

This reveals an important characteristic of the current encoder: it is \textbf{harmonic-dependent} rather than \textbf{kinematic-dependent}. Detection relies primarily on high-frequency structural resonances excited by the fault (bands 2--8), not on the kinematic fault frequencies themselves (band~1). A kinematic-dependent encoder using logarithmically-spaced (e.g., Mel-scale) or bearing-geometry-aware band boundaries would place dedicated resolution around the BPFO/BPFI/BSF frequencies, but would require prior knowledge of the bearing model.

\textbf{Implication.} In practice, a monitoring system is calibrated on a specific machine at its operating load. The cross-load test represents a stress test of generalization. When calibration and testing are at the same load, NATIVA achieves AUC~$> 0.99$ consistently.

\subsection{Ablation: Kuramoto Phase Coherence}

We ran all 36 CWRU conditions with Kuramoto disabled (\texttt{use\_kuramoto=False}), keeping all other parameters identical. When Kuramoto is off, the phase coherence $\phi_j$ defaults to 1.0 for all neurons, meaning STDP learning rate is no longer modulated by phase alignment.

\textbf{Result: $\Delta = 0.000$ across all 36 conditions.} AUC-ROC is identical with and without Kuramoto, to 4 decimal places, at every load and every fault condition.

\begin{table}[h]
\centering
\caption{Kuramoto ablation results}
\label{tab:ablation}
\begin{tabular}{cccc}
\toprule
\textbf{Load} & \textbf{AUC (ON)} & \textbf{AUC (OFF)} & $\mathbf{\Delta}$ \\
\midrule
0HP & 0.999 & 0.999 & 0.000 \\
1HP & 0.823 & 0.823 & 0.000 \\
2HP & 0.985 & 0.985 & 0.000 \\
3HP & 0.999 & 0.999 & 0.000 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Interpretation.} Kuramoto modulates only STDP learning rates via $\phi_j$, but since STDP converges quickly on the relatively simple 8-dimensional input ($\sim$237 normal windows), the phase coherence modulation has no measurable effect on the learned weight matrix. This finding does not invalidate Kuramoto as a theoretical component---it may play a role in higher-dimensional tasks, noisy environments, or continual learning scenarios. However, for the specific CWRU bearing fault detection task, Kuramoto is inert and could be removed without performance impact.

%-----------------------------------------------------------------------
\section{Discussion}
%-----------------------------------------------------------------------

\subsection{NATIVA vs.\ Conventional Unsupervised Methods}

The Dense Autoencoder, Isolation Forest, and One-Class SVM all achieve AUC~$\approx$~1.000 on the same 36 CWRU conditions (Table~\ref{tab:baselines}). This reveals an important insight: \textbf{the anomaly detection problem on CWRU, with hand-crafted FFT features, is trivially separable.}

NATIVA's mean AUC of 0.951 is lower because it does not operate on hand-crafted features. Its input pathway is: raw signal $\rightarrow$ STFT $\rightarrow$ 8 frequency bands $\rightarrow$ binary spikes. This spike encoding is an information bottleneck by design---it discards amplitude precision in exchange for a binary, event-driven representation compatible with neuromorphic hardware.

The comparison is therefore not ``NATIVA vs.\ Autoencoder'' but ``spike encoding vs.\ floating-point features.'' The relevant question is whether the 5\% AUC gap (0.95 vs 1.00) is an acceptable trade-off for the architectural advantages of an SNN: no floating-point MAC operations in the network, potential for sub-milliwatt power consumption on neuromorphic chips, and natural compatibility with event-driven sensors.

For the wake-up sensor use case (Section~V-B), 0.95 AUC with a spike-based architecture is more valuable than 1.00 AUC requiring a CPU and floating-point arithmetic, because the former can run continuously on a battery or energy harvester.

\subsection{The Delay-Line Coincidence (Jeffress Model) and the Elimination of FFT}

Following the limitation on the Paderborn dataset (which features distributed fatigue rather than localized spalling), we faced a fundamental limit: the damage signature is a periodic amplitude modulation hidden within broadband noise, without sharp impacts to trigger the Envelope encoder. Standard FFT approaches extract this trivially, but violate the SWaP constraints of our edge target.

To detect this hidden periodicity efficiently without frequency-domain transformations, we implemented a third neuromorphic column inspired by the \textbf{Jeffress Model of sound localization} (Delay-Line Coincidence). Rather than computing frequencies, the network measures temporal autocorrelation using binary shift registers:
\begin{enumerate}
    \item \textbf{Spike generation}: Raw signal $\rightarrow$ differential thresholding $\rightarrow$ binary stream.
    \item \textbf{Delay lines}: The stream is duplicated and delayed by varying steps $\Delta t$ using bitwise shifts.
    \item \textbf{Coincidence detection}: A LIF neuron fires only if a spike arrives simultaneously from the undelayed line and the delayed line (logical AND).
\end{enumerate}

A periodic fault (e.g., inner race fatigue at 150\,Hz $\approx$ 6.6\,ms period) causes a massive spike in coincidence detections precisely when the delay-line length matches the period $\Delta t = 6.6$\,ms. By wiring a LIF neuron to this specific delay-line, STDP learns the healthy periodicity of the bearing. When a fault emerges, the dominant delay-line shifts, prediction error explodes, and Free Energy spikes.

\textbf{Result on Paderborn (KA03)}: The Delay-Line column elevated detection on the periodic fatigue condition from AUC = 0.256 to \textbf{0.987}. Reducing a complex spectral analysis task to simple binary shifts (logical AND) for a few kilobytes of RAM ($<12$\,KB) proves that a neuromorphic approach can eliminate the need for floating-point FFTs in periodic defect detection.

\subsection{The Taxonomy of Physical Observability}

The cumulative results across CWRU and Paderborn using multiple encoding strategies elucidate a critical truth in unsupervised predictive maintenance: \textbf{algorithm performance is strictly bound by the physical observability of the defect.} We define a taxonomy of bearing faults based on their expression in the temporal vibration signal:
\begin{enumerate}
    \item \textbf{Impulsive (e.g., CWRU localized spalling):} Sharp, high-frequency resonance bursts. 
       \\* \textit{Observable via}: Time-domain Enveloping (Envelope V2).
       \\* \textit{Result}: AUC 1.000 (Fully resolved in 4.7\,KB RAM).
    \item \textbf{Periodic Non-Impulsive (e.g., Paderborn localized fatigue):} Low-frequency amplitude modulation buried in noise.
       \\* \textit{Observable via}: Delay-Line Coincidence (Jeffress Model) or FFT.
       \\* \textit{Result}: AUC 0.987 (Resolved in $<12$\,KB RAM).
    \item \textbf{Aperiodic Continuous (e.g., Paderborn distributed abrasion KA05):} Pink/white noise with shifting variance but no rhythmic signature.
       \\* \textit{Observable via}: None (with a single accelerometer).
       \\* \textit{Result}: The Mono-Sensor Physical Wall. 
\end{enumerate}

Aperiodic continuous faults expose the absolute limit of a single accelerometer. Extracting long-term variance drifts using an anomalous threshold inevitably triggers false alarms during normal operational load changes (e.g., VFD speed adjustments). Resolving Type 3 faults is not an algorithmic problem, but a multi-modal one, requiring external context (e.g., motor current or acoustics) to disambiguate load variations from frictional wear.

\subsection{The True ``Plug \& Learn'' Rupture}

Standard Edge AI architectures (TinyML) rely on ``frozen weights.'' A compressed Random Forest or CNN is trained offline on a cloud server and flashed to a microcontroller. As machinery ages and environmental conditions shift (Concept Drift), the static model degrades, leading to false alarms and requiring expensive re-acquisition of fault data and re-training.

NATIVA breaks this paradigm. Leveraging STDP and Active Inference, it learns and adapts entirely \textit{on-device}, without backpropagation. It acts as an autonomous ``Plug \& Learn'' sensor. Transferred to any healthy machine possessing similar impact physics, it calibrates its own Free Energy baseline locally within minutes, and continuously adapts its synaptic weights to slow environmental changes while remaining hypersensitive to abrupt mechanical anomalies.

\subsection{The Wake-Up Sensor Architecture}

We propose NATIVA not as a replacement for supervised systems, but as a complementary Level-1 filter in a two-tier architecture:
\begin{itemize}
    \item \textbf{Level 1 (NATIVA on edge)}: runs continuously, flags anomalies with high recall, minimal power
    \item \textbf{Level 2 (CNN/RF on cloud/gateway)}: activated only on Level-1 alert, performs precise fault classification
\end{itemize}

In our single-condition results, NATIVA in wake-up mode (recall~$\geq$~95\%) produces only 4 false positives across $\sim$374 normal test windows (FP rate = 1.1\%). The economic case: a false positive costs $\sim$15 minutes of technician time ($\sim$50\,\euro), while a missed fault can cost 150k\,\euro$+$ in unplanned downtime. At this FP rate, the annual cost of false alarms on a continuous monitoring system is negligible.

\subsection{Limitations}

\begin{enumerate}
    \item \textbf{1HP degradation}: 3 of 36 conditions fail (AUC~$< 0.77$). The multi-band encoder with linearly-spaced bands is not robust to all load-frequency interactions.
    \item \textbf{STFT computational cost}: The encoding stage requires floating-point FFT computation, partially negating the event-driven advantage of the SNN. The current architecture is hybrid.
    \item \textbf{No hardware benchmarks}: We have not measured latency or power consumption on embedded or neuromorphic hardware. The spike-based advantage is architectural but not empirically validated.
    \item \textbf{Single dataset}: Results on CWRU alone do not guarantee generalization. Validation on Paderborn~\cite{lessmeier2016} or MFPT datasets would strengthen the claims.
    \item \textbf{Minimal generative model}: The Free Energy module uses fixed precisions and a static prior. Adaptive precisions or Bernoulli likelihoods could improve sensitivity.
    \item \textbf{Kuramoto inert on CWRU}: The ablation study shows $\Delta=0.000$ across all 36 conditions. Kuramoto phase coherence does not measurably contribute to anomaly detection on this task.
\end{enumerate}

%-----------------------------------------------------------------------
\section{Conclusion}
%-----------------------------------------------------------------------

NATIVA demonstrates that a spiking neural network with biologically-inspired learning rules can detect bearing faults without labeled data, achieving a mean AUC-ROC of 0.95 across 36 CWRU conditions. The key engineering insight is that spike encoding must preserve global amplitude information through calibrated normalization---per-window normalization destroys the fault signal entirely.

The architecture's main strengths are its unsupervised nature, its temporal dynamics (surprise-based anomaly scoring), and its compatibility with edge deployment. Its main weaknesses are sensitivity to certain load-frequency interactions and the hybrid nature of the encoding pipeline.

Future work includes: (a)~logarithmic or bearing-geometry-aware frequency band spacing to address the 1HP limitation, (b)~replacing STFT with analog filter banks for a fully event-driven pipeline, (c)~hardware deployment on Loihi or SpiNNaker neuromorphic chips, (d)~comparison with LSTM autoencoders on identical CWRU conditions, and (e)~a dedicated ablation study of each architectural component on the anomaly detection task.

%-----------------------------------------------------------------------
% References
%-----------------------------------------------------------------------

\begin{thebibliography}{14}

\bibitem{henao2014}
H.~Henao \textit{et al.}, ``Trends in fault diagnosis for electrical machines: A review of diagnostic techniques,'' \textit{IEEE Ind.\ Electron.\ Mag.}, vol.~8, no.~2, pp.~31--42, 2014.

\bibitem{diehl2015}
P.~U.~Diehl and M.~Cook, ``Unsupervised learning of digit recognition using spike-timing-dependent plasticity,'' \textit{Front.\ Comput.\ Neurosci.}, vol.~9, 2015.

\bibitem{fell2011}
J.~Fell and N.~Axmacher, ``The role of phase synchronization in memory processes,'' \textit{Nat.\ Rev.\ Neurosci.}, vol.~12, no.~2, pp.~105--118, 2011.

\bibitem{friston2010}
K.~Friston, ``The free-energy principle: a unified brain theory?,'' \textit{Nat.\ Rev.\ Neurosci.}, vol.~11, no.~2, pp.~127--138, 2010.

\bibitem{smith2015}
W.~A.~Smith and R.~B.~Randall, ``Rolling element bearing diagnostics using the Case Western Reserve University data: A benchmark study,'' \textit{Mech.\ Syst.\ Signal Process.}, vol.~64--65, pp.~100--131, 2015.

\bibitem{kuramoto1984}
Y.~Kuramoto, \textit{Chemical Oscillations, Waves, and Turbulence}.\hskip 1em Berlin: Springer-Verlag, 1984.

\bibitem{liu2024}
Z.~Liu \textit{et al.}, ``Multi-scale residual attention SNN for bearing fault diagnosis,'' \textit{IEEE Access}, 2024.

\bibitem{loparo_cwru}
K.~A.~Loparo, ``Bearings vibration data set,'' Case Western Reserve University Bearing Data Center. [Online]. Available: \url{https://engineering.case.edu/bearingdatacenter}

\bibitem{liu2010neuromorphic}
S.-C.~Liu \textit{et al.}, ``Neuromorphic sensory systems,'' \textit{Curr.\ Opin.\ Neurobiol.}, vol.~20, no.~3, pp.~288--295, 2010.

\bibitem{gerstner2002}
W.~Gerstner and W.~M.~Kistler, \textit{Spiking Neuron Models: Single Neurons, Populations, Plasticity}.\hskip 1em Cambridge Univ.\ Press, 2002.

\bibitem{ott2008}
E.~Ott and T.~M.~Antonsen, ``Low dimensional behavior of large systems of globally coupled oscillators,'' \textit{Chaos}, vol.~18, 037113, 2008.

\bibitem{saari2019}
M.~Saari \textit{et al.}, ``Detection and classification of bearing faults using unsupervised machine learning,'' in \textit{Proc.\ IEEE ICIT}, 2019.

\bibitem{malhotra2016}
P.~Malhotra \textit{et al.}, ``LSTM-based encoder-decoder for multi-sensor anomaly detection,'' in \textit{Proc.\ ICML Anomaly Detection Workshop}, 2016.

\bibitem{lessmeier2016}
C.~Lessmeier \textit{et al.}, ``Condition monitoring of bearing damage in electromechanical drive systems by using motor current analysis (MCSA): Benchmark data set for data-driven classification,'' in \textit{Proc.\ Eur.\ Conf.\ PHM Society}, 2016.

\end{thebibliography}

\end{document}
